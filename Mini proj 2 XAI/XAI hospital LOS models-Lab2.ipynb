{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f84f1af2-7d01-40f3-90a1-812f1fc6c4a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Lab 2 - Comprehensive Explanations of Hospital LOS Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3c08c2-1a13-4af3-b7cb-17db74ea5c97",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# This code is based on Reta Giblin work for Project 1\n",
    "# This lab includes:\n",
    "#     Data explanation using SweetViz (this is at the end due to library compatibility issues)\n",
    "#     Global explanation using SHAP summary, PDP, ALE\n",
    "#     Local explantion using SHAP, LIME, counterfactuals\n",
    "\n",
    "# Attempts are also made to use the original data for explanation (rather than normalized/encoded data)\n",
    "# However, some explanation packages make this difficult (e.g. interpretML)\n",
    "\n",
    "# Reta GiblinM\n",
    "# DSBA-6010 Project 1\n",
    "# \n",
    "# Fall 2023\n",
    "# Data Preprocessing & Supervised Learning Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e2da5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Uncomment and run the following the first time and then put comment back on\n",
    "# Python 3.9 is recommended due to compatibility issues of various packages\n",
    "\n",
    "#%pip install interpret alibi shap dice-ml statsmodels seaborn ipywidgets sweetviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d07ebc",
   "metadata": {
    "tags": [
     "1"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, precision_score, recall_score, f1_score, precision_score, roc_auc_score, mean_squared_error, roc_curve, auc\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "pd.set_option('display.max_columns', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3a35d6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Read dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338d094e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#dtype_dict = {'dialysisrenalendstage': bool, 'asthma': bool, 'irondef': bool, 'pneum': bool, 'substancedependence': bool, \n",
    "#              'psychologicaldisordermajor': bool, 'depress': bool,  'psychother': bool,  'fibrosisandother': bool, \n",
    "#              'malnutrition': bool, 'hemo': bool,}\n",
    "dtype_dict = {'dialysisrenalendstage': int, 'asthma': int, 'irondef': int, 'pneum': int, 'substancedependence': int, \n",
    "              'psychologicaldisordermajor': int, 'depress': int,  'psychother': int,  'fibrosisandother': int, \n",
    "              'malnutrition': int, 'hemo': int,}\n",
    "\n",
    "# set up to read date fields as dates\n",
    "date_columns = ['vdate', 'discharged']\n",
    "los_data = pd.read_csv(\"LengthOfStay.csv\", dtype = dtype_dict, parse_dates=date_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a859894e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DATA PREPROCESSING/TRANSFORMATION\n",
    "los_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943af7a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a boolean column based on the 'gender' column\n",
    "los_data['is_female'] = los_data['gender'] == 'F'\n",
    "\n",
    "# Convert the boolean column to boolean data type\n",
    "#los_data['is_female'] = los_data['is_female'].astype(bool)\n",
    "los_data['is_female'] = los_data['is_female'].astype(int)\n",
    "\n",
    "# Transform lengthofstay to binary field\n",
    "# Create a Boolean field name longlengthstay\n",
    "# If lengthofstay Less that 7 days set longlengthofstay to 0 if >= 7 days set longlengthofstay to 1\n",
    "\n",
    "los_data['longlengthofstay'] = los_data['lengthofstay'] >= 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25f4963",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# DATA ENCODING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe5eb841",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create dummy variables for the facid column\n",
    "\n",
    "los_data = pd.get_dummies(los_data, columns=['facid'], prefix='facid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df440687",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Change rcount categorical value to an integer\n",
    "# Replace '5+' with '5' in the facid column\n",
    "los_data['rcount'] = los_data['rcount'].astype(str) \n",
    "los_data['rcount'] = los_data['rcount'].str.replace('5+', '5', regex=False)\n",
    "\n",
    "# Change type of facid to integer\n",
    "los_data['rcount'] = los_data['rcount'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2843df07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop unneeded columns\n",
    "column_to_drop = ['gender', 'lengthofstay','eid']\n",
    "los_data = los_data.drop(column_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b0119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# REMOVING OUTLIERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b44a82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to remove outliers based on Z-score\n",
    "def remove_outliers(df, columns, threshold=3):\n",
    "    z_scores = np.abs((df[columns] - df[columns].mean()) / df[columns].std())\n",
    "    df_no_outliers = df[(z_scores < threshold).all(axis=1)]\n",
    "    return df_no_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b63a9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Remove outliers for numerical data\n",
    "numeric_columns = ['hematocrit', 'neutrophils', 'sodium', 'glucose', 'bloodureanitro', 'creatinine', 'bmi', 'pulse','respiration']\n",
    "\n",
    "# Remove rows with outliers (Z-score threshold = 3)\n",
    "los_data = remove_outliers(los_data, numeric_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd9969a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Engineer vdate date\n",
    "\n",
    "# Extract vdate individual elements\n",
    "los_data['vdate_year'] = los_data['vdate'].dt.year\n",
    "los_data['vdate_month'] = los_data['vdate'].dt.month\n",
    "los_data['vdate_day'] = los_data['vdate'].dt.day\n",
    "los_data['vdate_day_of_week'] = los_data['vdate'].dt.dayofweek\n",
    "\n",
    "# Convert vdate elements to integers\n",
    "los_data['vdate_year'] = los_data['vdate_year'].astype(float)\n",
    "los_data['vdate_month'] = los_data['vdate_month'].astype(float)\n",
    "los_data['vdate_day'] = los_data['vdate_day'].astype(float)\n",
    "los_data['vdate_day_of_week'] = los_data['vdate_day_of_week'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c901b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Feature Engineer discharged date\n",
    "\n",
    "# Extract vdate individual elements\n",
    "los_data['discharged_year'] = los_data['discharged'].dt.year\n",
    "los_data['discharged_month'] = los_data['discharged'].dt.month\n",
    "los_data['discharged_day'] = los_data['discharged'].dt.day\n",
    "los_data['discharged_day_of_week'] = los_data['discharged'].dt.dayofweek\n",
    "\n",
    "# Convert vdate elements to integers\n",
    "los_data['discharged_year'] = los_data['discharged_year'].astype(float)\n",
    "los_data['discharged_month'] = los_data['discharged_month'].astype(float)\n",
    "los_data['discharged_day'] = los_data['discharged_day'].astype(float)\n",
    "los_data['discharged_day_of_week'] = los_data['discharged_day_of_week'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76bcb31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Drop original vdate and discharge fields\n",
    "column_to_drop = ['vdate', 'discharged']\n",
    "los_data = los_data.drop(column_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce3f765",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create heatmap of multicollinearity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "800151dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corr_matrix = los_data[['hematocrit', 'neutrophils', 'sodium', 'glucose', 'bloodureanitro', 'creatinine', 'bmi', 'pulse','respiration',\n",
    "                       'vdate_year', 'vdate_month', 'vdate_day', 'vdate_day_of_week', \n",
    "                        'discharged_year', 'discharged_month', 'discharged_day', 'discharged_day_of_week' ]].corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Multicollinearity Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22d58c0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# vdate and discharge date are too correlated\n",
    "# will not include vdate in model\n",
    "# Drop unneeded vdate\n",
    "\n",
    "column_to_drop = ['vdate_year', 'vdate_month', 'vdate_day', 'vdate_day_of_week']\n",
    "los_data = los_data.drop(column_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d485fef9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Rerun correlation matrix\n",
    "corr_matrix = los_data[['hematocrit', 'neutrophils', 'sodium', 'glucose', 'bloodureanitro', 'creatinine', 'bmi', 'pulse','respiration', \n",
    "                        'discharged_year', 'discharged_month', 'discharged_day', 'discharged_day_of_week']].corr()\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Multicollinearity Heatmap')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ef2bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "los_data['longlengthofstay']=los_data['longlengthofstay'].astype('int')\n",
    "los_data['facid_A']=los_data['facid_A'].astype('int')\n",
    "los_data['facid_B']=los_data['facid_B'].astype('int')\n",
    "los_data['facid_C']=los_data['facid_C'].astype('int')\n",
    "los_data['facid_D']=los_data['facid_D'].astype('int')\n",
    "los_data['facid_E']=los_data['facid_E'].astype('int')\n",
    "\n",
    "los_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d368756",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Ended up dropping discharged date too as was giving errors in model\n",
    "column_to_drop = ['discharged_year', 'discharged_month', 'discharged_day', 'discharged_day_of_week']\n",
    "los_data = los_data.drop(column_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8eaa9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set up scaling and encoding transformers\n",
    "# los_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7a1d5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get all the feature related names and indices for later use\n",
    "\n",
    "feature_names = los_data.columns.tolist()\n",
    "feature_names.remove('longlengthofstay')\n",
    "\n",
    "target_names = ['Extended LOS']\n",
    "\n",
    "num_col_names = ['rcount', 'hematocrit', 'neutrophils', 'sodium', 'glucose', 'bloodureanitro', 'creatinine', \n",
    "                    'bmi', 'pulse', 'respiration']\n",
    "num_col_indice = [feature_names.index(nc) for nc in num_col_names] # need this to fit alibi needs\n",
    "\n",
    "cat_col_names = [fn for fn in feature_names if fn not in num_col_names]\n",
    "cat_col_indice = [feature_names.index(cc) for cc in cat_col_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ee36ca-ab85-48e7-8ee2-fc485c1a66b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the data scaling preprocessors \n",
    "\n",
    "scaler = MinMaxScaler() #StandardScaler()\n",
    "\n",
    "# only numerical columns are scaled here\n",
    "# in the future, categorical columns will need encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', scaler, num_col_indice)\n",
    "    ],\n",
    "    remainder='passthrough',\n",
    "    sparse_threshold=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d66b0af7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d3d79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Split the dataset into features (X) and the target variable (y)\n",
    "\n",
    "X = los_data[feature_names] \n",
    "y = los_data['longlengthofstay']\n",
    "\n",
    "# Split the data into a training set and a testing\n",
    "X_train_df, X_test_df, y_train_df, y_test_df = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "#\n",
    "#\n",
    "# Turn these into numpy arrays because alibi works with numpy array only\n",
    "#\n",
    "#\n",
    "X_train, X_test, y_train, y_test = X_train_df.to_numpy(), X_test_df.to_numpy(), y_train_df.to_numpy(), y_test_df.to_numpy()\n",
    "\n",
    "# Fit preprocessing transformations to the entire data set\n",
    "# This is important especially when categorical data is involved\n",
    "preprocessor.fit(X.to_numpy())\n",
    "\n",
    "# Transform both train and test data\n",
    "X_train_tf = preprocessor.transform(X_train)\n",
    "X_test_tf = preprocessor.transform(X_test)\n",
    "\n",
    "#*************************************************\n",
    "# In summary, the following are finalized data\n",
    "#    - DataFrame's\n",
    "#                X, y \n",
    "#                X_train_df, X_test_df, y_train_df, y_test_df\n",
    "#    - numpy array's\n",
    "#                X_train, X_test, y_train, y_test\n",
    "#    - normalized/encoded numpy array's\n",
    "#                X_train_tf, X_test_tf\n",
    "#**************************************************"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8462748f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## RUN SUPERVISED LEARNING MODELS\n",
    "##\n",
    "## ***NOTE: From here on all train and test data are in numpy format (no longer DataFrame)\n",
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7cf93c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d92e6bf-b675-48ff-9848-040558737a0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a logistic regression model\n",
    "logistic_regression = LogisticRegression(max_iter=5000)\n",
    "\n",
    "# Fit the model to the training data\n",
    "logistic_regression.fit(X_train_tf, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_LR = logistic_regression.predict(X_test_tf)\n",
    "\n",
    "# Probability of long stay\n",
    "y_proba_LR = logistic_regression.predict_proba(X_test_tf)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef01e914",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate model statistics\n",
    "\n",
    "# Calculate accuracy and other metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_LR)\n",
    "report = classification_report(y_test, y_pred_LR)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_LR)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(y_test, y_pred_LR)\n",
    "recall = recall_score(y_test, y_pred_LR)\n",
    "f1 = f1_score(y_test, y_pred_LR)\n",
    "\n",
    "# Calculate AUC (Area Under the ROC Curve)\n",
    "auc_LR = roc_auc_score(y_test, y_proba_LR)\n",
    "\n",
    "# Get coefficients and feature names\n",
    "coefficients = logistic_regression.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc670db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Print the model statististics - Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d414f8b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Accuracy: {accuracy}')\n",
    "print('\\nClassification Report:\\n', report)\n",
    "print('Confusion Matrix:\\n', conf_matrix)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "print(f\"AUC: {auc_LR:.2f}\")\n",
    "\n",
    "# Print the model's coefficients and intercept\n",
    "print(\"Features:\", feature_names)\n",
    "print(\"Coefficients:\", logistic_regression.coef_)\n",
    "print(\"Intercept:\", logistic_regression.intercept_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712acb23",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Since the dataset is highly imbalanced, data balancing should be added in the future\n",
    "# Furthermore, the area of precision recall curve (AUPRC) is a better indicator of \n",
    "# prediction performance. In sklearn, this value is estimated by the following function.\n",
    "\n",
    "from sklearn.metrics import average_precision_score\n",
    "auprc = average_precision_score(y_test, y_proba_LR)\n",
    "print(f\"AUPRC: {auprc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569e1d77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Gradiant Boosting Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d1da9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a Gradient Boosting Classifier\n",
    "gradient_boosting_classifier = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, max_depth=3, min_samples_leaf=2, min_samples_split=10)\n",
    "\n",
    "# Fit the model to the training data (Note: the data are in numpy format, which is needed by XAI packages)\n",
    "gradient_boosting_classifier.fit(X_train_tf, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred_GB = gradient_boosting_classifier.predict(X_test_tf)\n",
    "\n",
    "# Probability of long stay\n",
    "y_proba_GB = gradient_boosting_classifier.predict_proba(X_test_tf)[:, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b90ca71-a076-4415-a6b6-707e6df9a34a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#************\n",
    "# Because DiCE library doesn't accept numpy, so fit another GB model here using DataFrame training data\n",
    "#************\n",
    "gbc_df = GradientBoostingClassifier(n_estimators=500, learning_rate=0.2, max_depth=3, min_samples_leaf=2, min_samples_split=10)\n",
    "\n",
    "# Fit the model to the training data using DataFrame data, and no standardization/encoding is needed\n",
    "# since the tree models don't really need them\n",
    "gbc_df.fit(X_train_df, y_train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f5ab6f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate model statistics\n",
    "\n",
    "# Calculate accuracy and other metrics\n",
    "accuracy = accuracy_score(y_test, y_pred_GB)\n",
    "report = classification_report(y_test, y_pred_GB)\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_GB)\n",
    "\n",
    "# Calculate precision, recall, and F1-score\n",
    "precision = precision_score(y_test, y_pred_GB)\n",
    "recall = recall_score(y_test, y_pred_GB)\n",
    "f1 = f1_score(y_test, y_pred_GB)\n",
    "\n",
    "# Calculate AUC (Area Under the ROC Curve)\n",
    "auc_GB = roc_auc_score(y_test, y_proba_GB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bdba16",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'Accuracy: {accuracy}')\n",
    "print('\\nClassification Report:\\n', report)\n",
    "print('Confusion Matrix:\\n', conf_matrix)\n",
    "print(f\"Precision: {precision:.2f}\")\n",
    "print(f\"Recall: {recall:.2f}\")\n",
    "print(f\"F1-score: {f1:.2f}\")\n",
    "print(f\"AUC: {auc_GB:.2f}\")\n",
    "#print(\"Feature Importance:\", feature_importance)\n",
    "\n",
    "# Print the model's coefficients and intercept\n",
    "print(\"Features: \", feature_names)\n",
    "print(\"Feature Importance: \", gradient_boosting_classifier.feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bb89d2c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "auprc = average_precision_score(y_test, y_proba_GB)\n",
    "print(f\"AUPRC: {auprc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701a7816-1427-43a4-bbec-b30595ebac4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The gbc_df model should perform similarily. Let's do a sanity check here.\n",
    "print(f\"AUPRC for gbc_df model: {average_precision_score(y_test_df, gbc_df.predict_proba(X_test_df)[:,1])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0210d203-d78d-43a8-9ffa-e4349cdeca1e",
   "metadata": {},
   "source": [
    "# Explainable AI Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36541272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "### XAI libraries - SHAP and InterpretML (with SHAP)\n",
    "import shap\n",
    "from interpret import show\n",
    "from interpret.blackbox import ShapKernel\n",
    "from interpret.blackbox import LimeTabular\n",
    "from interpret.blackbox import PartialDependence\n",
    "#gradient_boosting_classifier.predict_proba([X_test.iloc[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276bbdd7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Local explanation using Kernel SHAP using interpret\n",
    "predict_fn = lambda x: gradient_boosting_classifier.predict_proba(preprocessor.transform(x))[:,1]\n",
    "gb_shap = ShapKernel(predict_fn, shap.sample(X_train, 100), feature_names=feature_names)\n",
    "gb_shap_local = gb_shap.explain_local(X_test[:100], y_test[:100])\n",
    "show(gb_shap_local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecc2dc1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Local explanation with LIME using interpret\n",
    "gb_lime = LimeTabular(predict_fn, X_train, feature_names=feature_names)\n",
    "gb_lime_local = gb_lime.explain_local(X_test[0:100], y_test[0:100])\n",
    "show(gb_lime_local)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2db922d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Global explanation with PDP using interpret\n",
    "predict_fn = lambda x: gradient_boosting_classifier.predict_proba(preprocessor.transform(x))[:, 1]\n",
    "gb_pdp = PartialDependence(predict_fn, X_train, feature_names=feature_names)\n",
    "gb_pdp_global = gb_pdp.explain_global()\n",
    "show(gb_pdp_global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec10710",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Explanation using the SHAP library directly\n",
    "# The first parameter must be the model itself. We can't use the trick of predict_fn here.\n",
    "# Fortunately, for tree models, we shouldn't have used data normalization anyway.\n",
    "\n",
    "explainer = shap.TreeExplainer(gradient_boosting_classifier, feature_names=feature_names)\n",
    "# Calculate shapley values for test data\n",
    "#shap_values = explainer.shap_values(X_test)\n",
    "shap_values = explainer(X_test_tf[0:100])\n",
    "\n",
    "# Investigating the values (classification problem)\n",
    "# class 0 = contribution to class 1\n",
    "# class 1 = contribution to class 2\n",
    "#print(shap_values[1].shape)\n",
    "#shap_values\n",
    "\n",
    "explainer.expected_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48360a9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SHAP library visualization\n",
    "\n",
    "shap.initjs()\n",
    "\n",
    "# Local explanation using Force plot\n",
    "\n",
    "shap.plots.force(shap_values[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d39da610",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Local explanation using SHAP's Water Fall plot \n",
    "\n",
    "shap.plots.waterfall(shap_values[33])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a452c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Global explanation using SHAP Feature Summary\n",
    "\n",
    "shap.plots.beeswarm(shap_values)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72945031",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# XAI library for counterfactual analysis using DiCE\n",
    "\n",
    "import dice_ml\n",
    "\n",
    "data_dice = dice_ml.Data(dataframe=pd.concat([X_train_df, y_train_df], axis=1), \n",
    "                         continuous_features=num_col_names,\n",
    "                         outcome_name='longlengthofstay')\n",
    "model_dice = dice_ml.Model(model=gbc_df,\n",
    "                           backend='sklearn')\n",
    "exp_dice = dice_ml.Dice(data_dice, model_dice, method='random')\n",
    "\n",
    "cf = exp_dice.generate_counterfactuals(X_test_df[0:1], total_CFs=10, desired_class='opposite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b64c427b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Displaying counterfactuals as a dataframe\n",
    "\n",
    "cf.visualize_as_dataframe(show_only_changes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15dca95-0148-4290-9bc9-c1d690236d13",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PDP, ICE, ALE and many other methods using the alibi XAI library\n",
    "\n",
    "# PDP using alibi\n",
    "\n",
    "import alibi.explainers as aliexp\n",
    "predict_fn = lambda x: gradient_boosting_classifier.predict_proba(preprocessor.transform(x))[:, 1]\n",
    "pd_explainer = aliexp.PartialDependence(predictor=predict_fn,\n",
    "                              feature_names=feature_names,\n",
    "                                target_names=target_names,\n",
    "                                categorical_names=cat_col_names)\n",
    "pd_exp = pd_explainer.explain(X=X_test,\n",
    "                        kind='average')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c03d74-c9c4-4101-b4ec-7253816fbf06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%xmode minimal\n",
    "\n",
    "fig, axs = plt.subplots(3, 2, figsize=(7.5, 10), sharey='none', layout='constrained')\n",
    "aliexp.plot_pd(exp=pd_exp, ax=axs, features=[0, 4, 5, 12, 14, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b4664",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ALE plot using alibi\n",
    "\n",
    "from alibi.explainers import ALE\n",
    "from alibi.explainers.ale import plot_ale\n",
    "\n",
    "predict_fn = lambda x: gradient_boosting_classifier.predict_proba(preprocessor.transform(x))[:,1]\n",
    "ale = ALE(predict_fn, feature_names=feature_names)\n",
    "exp = ale.explain(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f8f444",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "plot_ale(exp, n_cols=2, sharey=None, features=[0, 4, 5, 12, 14, 15], line_kw={'label': 'Probability of \"longlengthofstay\" class'}, fig_kw={'figheight': 10, 'figwidth': 7.5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedb4a61-6575-422e-b0b9-f18377825915",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Data explanation and visualization using SweetViz\n",
    "#%pip install sweetviz\n",
    "\n",
    "import sweetviz\n",
    "los_data_viz = sweetviz.analyze([los_data, 'LOS'], target_feat='longlengthofstay')\n",
    "los_data_viz.show_html('sweetviz-los.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "593683da-12df-493e-8e10-7475b7ca0129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
